if (!file.exists(csv_inside_zip)) {
unzip(
zip_file,
files = csv_inside_zip,
exdir = "."
)
}
# Define the file path where you want to save the CSV file
file_path <- "Raw Data/COMPASS_SmartChamber_Metadata_2022.csv"
# Check if the CSV file exists
if (file.exists(file_path)) {
# If it exists, read it back into R
metadata <- read.csv(file_path)
print("CSV file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
#create a metadata file from the Plot IDs
# pull timestart timestamps to put in the metadata and obs lengths
metadata <- alldat %>%
group_by(SN, Plot_ID) %>%
summarise(SN = first(InstrumentSerialNumber), Roberta => I included this to process more the one licor per time
############### only run this first part if you don't already have a meta-data file
#because if you do then all the data for each one will have a plot and you won't be able to change start times
#create unique Plot ID for each measurement:
#detach("package:data.table", unload=TRUE)
alldat$month <- as.character(lubridate::month(alldat$TIMESTAMP, label=TRUE, abbr=TRUE)) #will not run if you have data.table loaded # Roberta -> solved with "lubridate::"
alldat$year <- year(alldat$TIMESTAMP)
alldat <- alldat %>%
mutate(Plot_ID = str_c(pmin(month, RepNum),
pmax(month, RepNum),  sep = '_'))
alldat <- alldat %>%
mutate(Plot_ID = str_c(pmin(Plot_ID, label),
pmax(Plot_ID, label),  sep = '_'))
#this needs to be PLot ID so that later we ID plot from metadata and not the metadata row
# Path inside the zip
zip_file <- "Raw Data_LE_2022.zip"
csv_inside_zip <- "Raw Data/COMPASS_SmartChamber_Metadata_2022.csv"
# Make sure the destination folder exists
dir.create("Raw Data", showWarnings = FALSE)
# Extract only the metadata CSV if it doesn't already exist
if (!file.exists(csv_inside_zip)) {
unzip(
zip_file,
files = csv_inside_zip,
exdir = "."
)
}
# Define the file path where you want to save the CSV file
file_path <- "Raw Data/COMPASS_SmartChamber_Metadata_2022.csv"
# Check if the CSV file exists
if (file.exists(file_path)) {
# If it exists, read it back into R
metadata <- read.csv(file_path)
print("CSV file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
#create a metadata file from the Plot IDs
# pull timestart timestamps to put in the metadata and obs lengths
metadata <- alldat %>%
group_by(SN, Plot_ID) %>%
summarise(SN = first(InstrumentSerialNumber), #Roberta => I included this to process more the one licor per time
Date = format(min(TIMESTAMP), "%Y-%m-%d"),
Time_start = format(min(TIMESTAMP), "%H:%M:%S"),
First_Timestamp = min(TIMESTAMP),  # Minimum (earliest) timestamp for each Plot_ID
Last_Timestamp = max(TIMESTAMP),   # Maximum (latest) timestamp for each Plot_ID
Total_Time = as.numeric(difftime(max(TIMESTAMP), min(TIMESTAMP), units = "secs")),  # Time difference in seconds
) %>%
ungroup() %>%
mutate(Total_Time = pmin(Total_Time, 60),
drop = FALSE,# Add a 'drop' column with 'FALSE' for every row
Notes = "")
#mutate(notes = " ")  # Add a 'drop' column with 'FALSE' for every row
#mutate(Deadband = 5)
#mutate(Plot = Plot_ID) #think need this because we need to make a plot column
# Write df_unique to CSV
write.csv(metadata, file = file_path, row.names = FALSE)
print("CSV file does not exist. It has been created.")
}
# Check if the CSV file exists
if (file.exists(file_path)) {
# If it exists, read it back into R
metadata <- read.csv(file_path)
print("CSV file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
#create a metadata file from the Plot IDs
# pull timestart timestamps to put in the metadata and obs lengths
metadata <- alldat %>%
group_by(SN, Plot_ID) %>%
summarise(SN = first(InstrumentSerialNumber), #Roberta => I included this to process more the one licor per time
Date = format(min(TIMESTAMP), "%Y-%m-%d"),
Time_start = format(min(TIMESTAMP), "%H:%M:%S"),
First_Timestamp = min(TIMESTAMP),  # Minimum (earliest) timestamp for each Plot_ID
Last_Timestamp = max(TIMESTAMP),   # Maximum (latest) timestamp for each Plot_ID
Total_Time = as.numeric(difftime(max(TIMESTAMP), min(TIMESTAMP), units = "secs")),  # Time difference in seconds
) %>%
ungroup() %>%
mutate(Total_Time = pmin(Total_Time, 60),
drop = FALSE,# Add a 'drop' column with 'FALSE' for every row
Notes = "")
#mutate(notes = " ")  # Add a 'drop' column with 'FALSE' for every row
#mutate(Deadband = 5)
#mutate(Plot = Plot_ID) #think need this because we need to make a plot column
# Write df_unique to CSV
write.csv(metadata, file = file_path, row.names = FALSE)
print("CSV file does not exist. It has been created.")
}
############### only run this first part if you don't already have a meta-data file
#because if you do then all the data for each one will have a plot and you won't be able to change start times
#create unique Plot ID for each measurement:
#detach("package:data.table", unload=TRUE)
alldat$month <- as.character(lubridate::month(alldat$TIMESTAMP, label=TRUE, abbr=TRUE)) #will not run if you have data.table loaded # Roberta -> solved with "lubridate::"
alldat$year <- year(alldat$TIMESTAMP)
alldat <- alldat %>%
mutate(Plot_ID = str_c(pmin(month, RepNum),
pmax(month, RepNum),  sep = '_'))
alldat <- alldat %>%
mutate(Plot_ID = str_c(pmin(Plot_ID, label),
pmax(Plot_ID, label),  sep = '_'))
#this needs to be PLot ID so that later we ID plot from metadata and not the metadata row
# Path inside the zip
zip_file <- "Raw Data_LE_2022.zip"
csv_inside_zip <- "Raw Data/COMPASS_SmartChamber_Metadata_2022.csv"
# Make sure the destination folder exists
dir.create("Raw Data", showWarnings = FALSE)
# Extract only the metadata CSV if it doesn't already exist
if (!file.exists(csv_inside_zip)) {
unzip(
zip_file,
files = csv_inside_zip,
exdir = "."
)
}
# Define the file path where you want to save the CSV file
file_path <- "Raw Data/COMPASS_SmartChamber_Metadata_2022.csv"
# Check if the CSV file exists
if (file.exists(file_path)) {
# If it exists, read it back into R
metadata <- read.csv(file_path)
print("CSV file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
#create a metadata file from the Plot IDs
# pull timestart timestamps to put in the metadata and obs lengths
metadata <- alldat %>%
group_by(InstrumentSerialNumber, Plot_ID) %>%
summarise(SN = first(InstrumentSerialNumber), #Roberta => I included this to process more the one licor per time
Date = format(min(TIMESTAMP), "%Y-%m-%d"),
Time_start = format(min(TIMESTAMP), "%H:%M:%S"),
First_Timestamp = min(TIMESTAMP),  # Minimum (earliest) timestamp for each Plot_ID
Last_Timestamp = max(TIMESTAMP),   # Maximum (latest) timestamp for each Plot_ID
Total_Time = as.numeric(difftime(max(TIMESTAMP), min(TIMESTAMP), units = "secs")),  # Time difference in seconds
) %>%
ungroup() %>%
mutate(Total_Time = pmin(Total_Time, 60),
drop = FALSE,# Add a 'drop' column with 'FALSE' for every row
Notes = "")
#mutate(notes = " ")  # Add a 'drop' column with 'FALSE' for every row
#mutate(Deadband = 5)
#mutate(Plot = Plot_ID) #think need this because we need to make a plot column
# Write df_unique to CSV
write.csv(metadata, file = file_path, row.names = FALSE)
print("CSV file does not exist. It has been created.")
}
# line 189 :: problems with weird 1017190 second for Total time generated in the metadata for TG 01028 days: 7-7-2022,7-8-2022,7-12-2022,7-14-2022,7-19-2022,-> need to check why later, but I am fixing the timestamp at 120 sec for now
#remove any metadata rows that have NAs for the PlotID
metadata <- metadata %>%
filter(!is.na(Plot_ID)) %>%
filter(str_detect(Plot_ID, paste(c("OWC", "CRC", "PTR", "MSM"), collapse = "|")))
alldat<-alldat%>% filter(!is.na(label))
# this next part is not working because of overlapping timestamp among >1 licor instruments.
#match the altered metadata to the concentration data with fluxfinder ffi_metadata_match
# alldat$metadat_row <- ffi_metadata_match(
#   data_timestamps = alldat$TIMESTAMP,
#   start_dates = metadata$Date,
#   start_times = metadata$Time_start,
#   obs_lengths = metadata$Total_Time)
# head(alldat)
# matching SN
alldat <- alldat %>%
group_by(SN) %>%
mutate(
metadat_row = ffi_metadata_match(
data_timestamps = TIMESTAMP,
start_dates     = metadata$Date[metadata$SN == first(SN)],
start_times     = metadata$Time_start[metadata$SN == first(SN)],
obs_lengths     = metadata$Total_Time[metadata$SN == first(SN)]
)
) %>%
ungroup()
View(alldat)
# matching SN
alldat <- alldat %>%
rename(SN=InstrumentSerialNumber)%>%
group_by(SN=) %>%
mutate(
metadat_row = ffi_metadata_match(
data_timestamps = TIMESTAMP,
start_dates     = metadata$Date[metadata$SN == first(SN)],
start_times     = metadata$Time_start[metadata$SN == first(SN)],
obs_lengths     = metadata$Total_Time[metadata$SN == first(SN)]
)
) %>%
ungroup()
# matching SN
alldat <- alldat %>%
rename(SN=InstrumentSerialNumber)%>%
group_by(SN) %>%
mutate(
metadat_row = ffi_metadata_match(
data_timestamps = TIMESTAMP,
start_dates     = metadata$Date[metadata$SN == first(SN)],
start_times     = metadata$Time_start[metadata$SN == first(SN)],
obs_lengths     = metadata$Total_Time[metadata$SN == first(SN)]
)
) %>%
ungroup()
alldat <- alldat %>%
left_join(
metadata %>% mutate(metadat_row = row_number()),
by = c("SN", "metadat_row")
)
#alldat$Deadband <- metadata$Deadband[alldat$metadat_row] #flux calcs function cannot handle a column as a deadband and that is a problem to fix
alldat$Plot <- metadata$Plot_ID[alldat$metadat_row]
#change the units
alldat$CH4_nmol <- ffi_ppb_to_nmol(alldat$ch4,
volume = (alldat$TotalVolume / 1000000), # have to divide to get to m3
temp = alldat$chamber_t)    # degrees C
#> Assuming atm = 101325 Pa
#> Using R = 8.31446261815324 m3 Pa K-1 mol-1
alldat$CH4_nmol_m2 <- alldat$CH4_nmol / 0.0318 #adjust for the area
#need to create a new line that would say only calculate the flux if the drop column is false
#before we add this we need to figure out how to drop except for the last flux taken
#calculate fluxes for all other sites
fluxes <- ffi_compute_fluxes(alldat,
group_column = "Plot",
time_column = "TIMESTAMP",
gas_column = "CH4_nmol_m2",
dead_band = 5)
head(fluxes)
allflux <- fluxes
fluxes <- fluxes %>%
mutate(
linear_ok = lin_r.squared >= 0.8,
nonlinear_ok = HM81_r.squared >= 0.8,
accepted = linear_ok | (!linear_ok & nonlinear_ok)
)
summary_df <- fluxes %>%
summarise(
total_obs = n(),
linear_ok_count = sum(linear_ok, na.rm = TRUE),
nonlinear_only_count = sum(!linear_ok & nonlinear_ok, na.rm = TRUE),
rejected_count = sum(!linear_ok & !nonlinear_ok, na.rm = TRUE),
pct_linear_ok = 100 * linear_ok_count / total_obs,
pct_nonlinear_only = 100 * nonlinear_only_count / total_obs,
pct_rejected = 100 * rejected_count / total_obs
)
summary_df
#total_obs linear_ok_count nonlinear_only_count rejected_count pct_linear_ok pct_nonlinear_only pct_rejected
#1       643             531                   12             20      82.58165           1.866252      3.11042
#so first we need to make it so that the fluxes that were retaken have similar names or some way to identify them
# -------------------------------
# 1. Clean fluxes
# -------------------------------
fluxes <- fluxes %>%
filter(str_detect(Plot, "CRC|MSM|PTR|OWC")) %>%
mutate(
Plot = sapply(str_split(Plot, "_"), function(parts) {
if(length(parts) >= 4) {
# Uppercase TR, UP, W and add underscore
parts[4] <- str_replace(parts[4], "(?i)^(TR|UP|W)", function(x) paste0(toupper(x), "_"))
}
if(length(parts) >= 5) {
# Separate replicate number and suffix
parts[5] <- str_replace(parts[5], "(\\d+)([A-Za-z]+)", "\\1_\\2")
}
paste(parts, collapse = "_")
})
)
# -------------------------------
# 2. Clean alldat
# -------------------------------
alldat <- alldat %>%
filter(str_detect(Plot, "CRC|MSM|PTR|OWC")) %>%
mutate(
Plot = str_replace_all(Plot, "-", "_"),
Plot = str_replace(Plot, "(?i)\\b(TR|UP|W)\\b", function(x) paste0(toupper(x), "_")),
Plot = str_replace_all(Plot, "(?<=\\d)([ABCD])\\b", tolower),
Plot = str_replace(Plot, "(\\d+)([a-zA-Z])$", "\\1_\\2"),
Plot = str_replace(Plot, "UP(\\d+)", "UP_\\1"),
Plot = str_replace_all(Plot, "\\btr\\b", "TR") %>%
str_replace_all("\\bup\\b", "UP") %>%
str_replace_all("\\bw\\b", "W")
)
# -------------------------------
# 3. Create ID columns function
# -------------------------------
create_IDs <- function(df) {
IDs <- df %>%
tidyr::separate(
col = Plot,
into = c("RepNumber", "Month", "Site", "Zone", "Replicate"),
fill = "right", extra = "merge"
) %>%
mutate(
# If columns are missing, fill with empty string
RepNumber = ifelse(is.na(RepNumber), "", RepNumber),
Month     = ifelse(is.na(Month), "", Month),
Site      = ifelse(is.na(Site), "", Site),
Zone      = ifelse(is.na(Zone), "", Zone),
Replicate = ifelse(is.na(Replicate), "", Replicate)
)
return(IDs)
}
# -------------------------------
# 4. Apply ID creation
# -------------------------------
fluxes <- create_IDs(fluxes)
alldat  <- create_IDs(alldat)
# -------------------------------
# 5. Extract base plot, replicate, suffix
# -------------------------------
fluxes <- fluxes %>%
mutate(
# base plot: everything except replicate/suffix
BasePlot  = paste(RepNumber, Month, Site, Zone, sep = "_"),
# replicate: numeric part
ReplicateNumber = as.numeric(str_extract(Replicate, "\\d+")),
# suffix: letter part
Suffix = str_extract(Replicate, "[a-zA-Z]$"),
Suffix = ifelse(is.na(Suffix), "", Suffix)
)
# -------------------------------
# 6. Keep only latest replicate (highest suffix)
# -------------------------------
allflux <- fluxes %>%
group_by(BasePlot, ReplicateNumber) %>%
arrange(Month, Site, Zone, ReplicateNumber, desc(Suffix), .by_group = TRUE) %>%
slice_head(n = 1) %>%
ungroup()
#take a look at the fluxes we calculated with R2
ggplot(allflux, aes(BasePlot, lin_flux.estimate, color = lin_r.squared)) +
geom_point(size=4) + theme_classic() + ylim(-50,2000) +
geom_linerange(aes(ymin = lin_flux.estimate- lin_flux.std.error,
ymax = lin_flux.estimate+ lin_flux.std.error)) +
scale_color_gradientn(colours = c("darkred",  "red", "white","lightblue", "darkblue"),
values = c(0, 0.5, 0.6, 0.7, 1)) +
ylab("CH4 flux (nmol/m2/s)")
#sort fluxes by R2
fluxes_good <- as.data.frame(subset(allflux, lin_r.squared >= .90, select = Plot:TIMESTAMP_max))
#take a look at the fluxes we calculated with R2
ggplot(allflux, aes(BasePlot, lin_flux.estimate, color = lin_r.squared)) +
geom_point(size=4) + theme_classic() + ylim(-50,2000) +
geom_linerange(aes(ymin = lin_flux.estimate- lin_flux.std.error,
ymax = lin_flux.estimate+ lin_flux.std.error)) +
scale_color_gradientn(colours = c("darkred",  "red", "white","lightblue", "darkblue"),
values = c(0, 0.5, 0.6, 0.7, 1)) +
ylab("CH4 flux (nmol/m2/s)")
#sort fluxes by R2
fluxes_good <- as.data.frame(subset(allflux, lin_r.squared >= .90, select = BasePlot:TIMESTAMP_max))
fluxes_lowr2 <- as.data.frame(subset(allflux, lin_r.squared < .90 & lin_flux.estimate < 1, select = BasePlot:TIMESTAMP_max))
fluxes_weird <- as.data.frame(subset(allflux, lin_r.squared < .90 & lin_flux.estimate > 1, select = BasePlot:TIMESTAMP_max))
#find fluxes that represent the first and fourth quartile of the data
low <- quantile(allflux$lin_flux.estimate, 0.05)
high <- quantile(allflux$lin_flux.estimate, 0.95)
fluxes_low <- as.data.frame(subset(allflux, lin_flux.estimate <= low, select = BasePlot:TIMESTAMP_max))
fluxes_high <- as.data.frame(subset(allflux, lin_flux.estimate >= high, select = BasePlot:TIMESTAMP_max))
tails <- rbind(fluxes_low, fluxes_high)
fluxes_good
low
high
fluxes_weird
#Loop to run five random plots subset above
for(P in tobechecked){
r1 <- subset(alldat, Plot == P)
p1 <- ggplot(r1, aes(as.POSIXct(TIMESTAMP), ch4)) + geom_point() +
scale_x_datetime(breaks = "15 sec") +
theme_classic() +
ylab(expression(paste( CH [4], " (ppb)"))) +
xlab("Time") + labs(title = P) +
geom_smooth()
print(p1)
}
#Loop to run five random plots subset above
for(P in tobechecked){
r1 <- subset(alldat, BasePlot == P)
p1 <- ggplot(r1, aes(as.POSIXct(TIMESTAMP), ch4)) + geom_point() +
scale_x_datetime(breaks = "15 sec") +
theme_classic() +
ylab(expression(paste( CH [4], " (ppb)"))) +
xlab("Time") + labs(title = P) +
geom_smooth()
print(p1)
}
View(metadata)
### pull a random sample of good fluxes to check and plot
good_plots <- unique(fluxes_good$BasePlot)
# take a random sample (FALSE so we don't get the same plot twice)
# set.seed(...) to guarantee reproducibility
good_sample <- sample(good_plots, 5, replace=FALSE)
#Loop to run five random plots subset above
for(P in good_sample){
r1 <- subset(alldat, BasePlot == P)
p1 <- ggplot(r1, aes(as.POSIXct(TIMESTAMP), ch4)) + geom_point() +
scale_x_datetime(breaks = "15 sec") +
theme_classic() +
ylab(expression(paste( CH [4], " (ppb)"))) +
xlab("Time") + labs(title = P) +
geom_smooth()
print(p1)
}
r1 <- subset(alldat, Plot == P)
p1 <- ggplot(r1, aes(as.POSIXct(TIMESTAMP), ch4)) + geom_point() +
scale_x_datetime(breaks = "15 sec") +
theme_classic() +
ylab(expression(paste( CH [4], " (ppb)"))) +
xlab("Time") + labs(title = P) +
geom_smooth()
### pull a random sample of good fluxes to check and plot
good_plots <- unique(fluxes_good$Plot)
good_plots
fluxes_good
alldat
str(alldat)
#Loop to run five random plots subset above
for(P in good_sample){
r1 <- subset(alldat, Plot_ID == P)
p1 <- ggplot(r1, aes(as.POSIXct(TIMESTAMP), ch4)) + geom_point() +
scale_x_datetime(breaks = "15 sec") +
theme_classic() +
ylab(expression(paste( CH [4], " (ppb)"))) +
xlab("Time") + labs(title = P) +
geom_smooth()
print(p1)
}
### pull a random sample of good fluxes to check and plot
good_plots <- unique(fluxes_good$BasePlot)
# take a random sample (FALSE so we don't get the same plot twice)
# set.seed(...) to guarantee reproducibility
good_sample <- sample(good_plots, 5, replace=FALSE)
#Loop to run five random plots subset above
for(P in good_sample){
r1 <- subset(alldat, BasePlot == P)
p1 <- ggplot(r1, aes(as.POSIXct(TIMESTAMP), ch4)) + geom_point() +
scale_x_datetime(breaks = "15 sec") +
theme_classic() +
ylab(expression(paste( CH [4], " (ppb)"))) +
xlab("Time") + labs(title = P) +
geom_smooth()
print(p1)
}
#Loop to run five random plots subset above
for(P in good_sample){
r1 <- subset(alldat, Plot == P)
p1 <- ggplot(r1, aes(as.POSIXct(TIMESTAMP), ch4)) + geom_point() +
scale_x_datetime(breaks = "15 sec") +
theme_classic() +
ylab(expression(paste( CH [4], " (ppb)"))) +
xlab("Time") + labs(title = P) +
geom_smooth()
print(p1)
}
####Do we just leave these alone because the fluxes are so low?####
### pull out fluxes with R2 < 0.9 and put them in a list
low_plots <- unique(fluxes_lowr2$Plot)
#Loop to run five random plots subset above
for(P in low_plots){
r1 <- subset(alldat, Plot == P)
p1 <- ggplot(r1, aes(TIMESTAMP, ch4)) + geom_point() + theme_classic() +
scale_x_datetime(breaks = "15 sec") +
ylab(expression(paste( CH [4], " (ppb)"))) +
xlab("Time") + labs(title = P) +
geom_smooth()
print(p1)
}
####################
low_plots <- unique(fluxes_lowr2$Plot)
#Loop to run five random plots subset above
for(P in low_plots){
r1 <- subset(alldat, Plot == P)
p1 <- ggplot(r1, aes(TIMESTAMP, ch4)) + geom_point() + theme_classic() +
scale_x_datetime(breaks = "15 sec") +
ylab(expression(paste( CH [4], " (ppb)"))) +
xlab("Time") + labs(title = P) +
geom_smooth()
print(p1)
}
### pull out fluxes with R2 < 0.9 and put them in a list
bad_plots <- unique(fluxes_weird$Plot)
#Loop to run five random plots subset above
for(P in bad_plots){
r1 <- subset(alldat, Plot == P)
p1 <- ggplot(r1, aes(TIMESTAMP, ch4)) + geom_point() + theme_classic() +
scale_x_datetime(breaks = "15 sec") +
ylab(expression(paste( CH [4], " (ppb)"))) +
xlab("Time") + labs(title = P) +
geom_smooth()
print(p1)
}
####################
p1 <- ggplot(allflux_filtered, aes(lin_flux.estimate, rob_flux.estimate, color = Plot)) +
geom_point(size=4) + geom_abline() +
theme_classic() +
labs(title = "A") +
ylab(expression(paste( CH [4], " Flux (nmol m"^-2* " s"^-1*") Robust Method"))) +
xlab(expression(paste( CH [4], " Flux (nmol m"^-2* " s"^-1*") Linear Method"))) +
guides(color = guide_legend(title = "Plot")) +
theme(legend.position="none") +
theme(axis.title.x = element_text(size=12), axis.text = element_text(size=12),
axis.title.y = element_text(size=12), legend.text=element_text(size=12),
panel.border = element_rect(colour = "black", fill=NA, linewidth =1),
aspect.ratio = 1)
